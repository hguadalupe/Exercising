{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8059cb",
   "metadata": {},
   "source": [
    "## Ex.1 Create a DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8b02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826ec8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "##This operation is for gathering words,bools and integers.\n",
    "#word_site = \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "#response = requests.get(word_site)\n",
    "#WORDS = response.content.splitlines()\n",
    "#randwords = random.choices(WORDS, k=20)\n",
    "#randInteg= random.sample(range(-10, 100), 20)\n",
    "#bools= [True,False]\n",
    "#randBool= np.random.choice(bools, size=20)\n",
    "\n",
    "randWords = ['sheriff', 'firm', 'first', 'carroll', 'taiwan', \n",
    "             'floor', 'dependent', 'earthquake', 'naples', 'kruger', \n",
    "             'chapter', 'lender','official','sheffield','ieee',\n",
    "             'moves','richmond','reader','adapted','well']\n",
    "randWords2 = ['build','infrastructure','allah','spin','meals','heart',\n",
    "            'mobile','first','charging','knowledgestorm','policies',\n",
    "            'brake','system','ross','superior','mastercard','saturday',\n",
    "            'resistant','anthony','investing']\n",
    "randInteg= [70, 79, -7, 66, 9, 63, 22, 78, \n",
    "            41, 32, 65, 54, 76, 83, 17, \n",
    "            -8, 28, 45, 15, 91]\n",
    "randBool= [ True, False,  True,  True,  True, False, \n",
    "           False,  True, False,True, False, False,\n",
    "           False,  True,  True,  True,  True,  True,False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f3a94ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>words2</th>\n",
       "      <th>integers</th>\n",
       "      <th>booleans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sheriff</td>\n",
       "      <td>build</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>firm</td>\n",
       "      <td>infrastructure</td>\n",
       "      <td>79</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>allah</td>\n",
       "      <td>-7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carroll</td>\n",
       "      <td>spin</td>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taiwan</td>\n",
       "      <td>meals</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     words          words2  integers  booleans\n",
       "0  sheriff           build        70      True\n",
       "1     firm  infrastructure        79     False\n",
       "2    first           allah        -7      True\n",
       "3  carroll            spin        66      True\n",
       "4   taiwan           meals         9      True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'words' : randWords ,'words2':randWords2, 'integers' : randInteg , 'booleans' : randBool}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce3be2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To export the DF.\n",
    "#df.to_csv(r'for_lambda_and_pyspark.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db36524",
   "metadata": {},
   "source": [
    "## Ex. 2 Iterate with Lambda\n",
    "It gets on well with map() for numeric vales and functions with strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c837f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(o,p):\n",
    "    if len(o) < 5 and p > 50 :\n",
    "        return 'tatum'\n",
    "    else:\n",
    "        return 'oooo'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9233e370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tatum'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunc(\"1323\",60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9da2a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['peter'] = df.apply(lambda x: \n",
    "                                myfunc(x['words'],x['integers']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6697df93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>words2</th>\n",
       "      <th>integers</th>\n",
       "      <th>booleans</th>\n",
       "      <th>peter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sheriff</td>\n",
       "      <td>build</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>firm</td>\n",
       "      <td>infrastructure</td>\n",
       "      <td>79</td>\n",
       "      <td>False</td>\n",
       "      <td>tatum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first</td>\n",
       "      <td>allah</td>\n",
       "      <td>-7</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carroll</td>\n",
       "      <td>spin</td>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taiwan</td>\n",
       "      <td>meals</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>floor</td>\n",
       "      <td>heart</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dependent</td>\n",
       "      <td>mobile</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>first</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>naples</td>\n",
       "      <td>charging</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kruger</td>\n",
       "      <td>knowledgestorm</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chapter</td>\n",
       "      <td>policies</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lender</td>\n",
       "      <td>brake</td>\n",
       "      <td>54</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>official</td>\n",
       "      <td>system</td>\n",
       "      <td>76</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sheffield</td>\n",
       "      <td>ross</td>\n",
       "      <td>83</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ieee</td>\n",
       "      <td>superior</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>moves</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>-8</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>richmond</td>\n",
       "      <td>saturday</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>reader</td>\n",
       "      <td>resistant</td>\n",
       "      <td>45</td>\n",
       "      <td>True</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adapted</td>\n",
       "      <td>anthony</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>oooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>well</td>\n",
       "      <td>investing</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>tatum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words          words2  integers  booleans  peter\n",
       "0      sheriff           build        70      True   oooo\n",
       "1         firm  infrastructure        79     False  tatum\n",
       "2        first           allah        -7      True   oooo\n",
       "3      carroll            spin        66      True   oooo\n",
       "4       taiwan           meals         9      True   oooo\n",
       "5        floor           heart        63     False   oooo\n",
       "6    dependent          mobile        22     False   oooo\n",
       "7   earthquake           first        78      True   oooo\n",
       "8       naples        charging        41     False   oooo\n",
       "9       kruger  knowledgestorm        32      True   oooo\n",
       "10     chapter        policies        65     False   oooo\n",
       "11      lender           brake        54     False   oooo\n",
       "12    official          system        76     False   oooo\n",
       "13   sheffield            ross        83      True   oooo\n",
       "14        ieee        superior        17      True   oooo\n",
       "15       moves      mastercard        -8      True   oooo\n",
       "16    richmond        saturday        28      True   oooo\n",
       "17      reader       resistant        45      True   oooo\n",
       "18     adapted         anthony        15     False   oooo\n",
       "19        well       investing        91     False  tatum"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369cea49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words       object\n",
       "words2      object\n",
       "integers     int64\n",
       "booleans      bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f884a8",
   "metadata": {},
   "source": [
    "## Ex. 3 Pyspark Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0baf386",
   "metadata": {},
   "source": [
    "http://localhost:8888/notebooks/Exercising/cheatsheets/pyspark/Pyspark_Iterate.ipynb\n",
    "\n",
    "http://localhost:8888/notebooks/Exercising/cheatsheets/pyspark/Pyspark_Jobs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f731c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/25 19:35:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create Pyspark enviroment\n",
    "import findspark\n",
    "findspark.init('/home/vieroh/apps/spark-3.3.0-bin-hadoop3-scala2.13') #Here your spark rute.\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b9ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 4)\n"
     ]
    }
   ],
   "source": [
    "df = (sc.read\n",
    "          .format(\"csv\")\n",
    "          .option('header', 'true')\n",
    "          .load(\"for_lambda_and_pyspark.csv\"))\n",
    "          \n",
    "print((df.count(), len(df.columns))) #Shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d220db0",
   "metadata": {},
   "source": [
    "DF to RDD fails:\n",
    "https://stackoverflow.com/questions/39521341/pyspark-error-attributeerror-sparksession-object-has-no-attribute-paralleli\n",
    "\n",
    "For initializing:\n",
    "https://www.elenacuoco.com/2016/08/28/pyspark-first-approaches-ml-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8f4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.sparkContext.parallelize([('Alex',21),('Bob',44)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88821b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0d5bd0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vieroh/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/home/vieroh/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/vieroh/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.RLock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd\u001b[38;5;241m=\u001b[39m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/context.py:674\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[0;32m--> 674\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/context.py:717\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/serializers.py:224\u001b[0m, in \u001b[0;36mBatchedSerializer.dump_stream\u001b[0;34m(self, iterator, stream)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator, stream):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/serializers.py:146\u001b[0m, in \u001b[0;36mFramedSerializer.dump_stream\u001b[0;34m(self, iterator, stream)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator, stream):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_with_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/serializers.py:156\u001b[0m, in \u001b[0;36mFramedSerializer._write_with_length\u001b[0;34m(self, obj, stream)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_with_length\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, stream):\n\u001b[0;32m--> 156\u001b[0m     serialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m serialized \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialized value should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/apps/spark-3.3.0-bin-hadoop3-scala2.13/python/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "rdd=sc.sparkContext.parallelize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64075a67",
   "metadata": {},
   "source": [
    "#### FIX THIS RDD Problem adapting the SparkConf() to your Hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing spark context correctly\n",
    "conf = SparkConf().setAppName('Mnist_Spark_MLP').set('spark.executor.memory', '8g').set('spark.driver.memory', '4g').setMaster('local[8]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c20ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29baf4ed",
   "metadata": {},
   "source": [
    "#### Foreach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e48a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLES\n",
    "#To get row by index. \n",
    "df.rdd.foreach(lambda x: \n",
    "              print(x[0], x[1], x[2], x[3])) \n",
    "\n",
    "#To get columns.  \n",
    "df.rdd.foreach(lambda x: print((x.column1, x.column2)))\n",
    "\n",
    "#Functions:\n",
    "def f(row):\n",
    "    print(row.name)\n",
    "\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf703ba7",
   "metadata": {},
   "source": [
    "#### Map()\n",
    "Recomiendo este Link:\n",
    "https://sparkbyexamples.com/pandas/pandas-apply-function-usage-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLES\n",
    "# Apply to multiple columns\n",
    "df[['A','B']] = df[['A','B']].apply(function_with_two_variables)\n",
    "\n",
    "# Refering columns by index.\n",
    "rdd=df.rdd.map(lambda x: \n",
    "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
    "    )  \n",
    "df2=rdd.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df2.show()\n",
    "\n",
    "# Referring Column Names (another example)\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc373655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c494d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57bc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a78a6b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
