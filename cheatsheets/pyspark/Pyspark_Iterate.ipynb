{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee054d7",
   "metadata": {},
   "source": [
    "#  Iteration in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e0190",
   "metadata": {},
   "source": [
    "RDDs:\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "Iterations:\n",
    "https://sparkbyexamples.com/pyspark/pyspark-loop-iterate-through-rows-in-dataframe/\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-map-transformation/\n",
    "\n",
    "https://sparkbyexamples.com/spark/spark-map-vs-mappartitions-transformation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17d1dd",
   "metadata": {},
   "source": [
    "PySpark map (map()) is an RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD. \n",
    "\n",
    "map() and foreach() are the most used functions for RDD iterating in Pyspark.\n",
    "\n",
    "Mostly for simple computations, instead of iterating through using map() and foreach(), you should use either DataFrame select() or DataFrame withColumn() in conjunction with PySpark SQL functions.\n",
    "\n",
    "RDD map() transformation is used to apply any complex operations like adding a column, updating a column, transforming the data e.t.c, the output of map transformations would always have the same number of records as input.\n",
    "\n",
    "    Note1: DataFrame doesn’t have map() transformation to use with DataFrame hence you need to DataFrame to RDD first.\n",
    "    Note2: If you have a heavy initialization use PySpark mapPartitions() transformation instead of map(), as with mapPartitions() heavy initialization executes only once for each partition instead of every record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91370434",
   "metadata": {},
   "source": [
    "### Map() examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d723c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refering columns by index.\n",
    "rdd=df.rdd.map(lambda x: \n",
    "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
    "    )  \n",
    "df2=rdd.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089fef8",
   "metadata": {},
   "source": [
    "The above example iterates through every row in a DataFrame by applying transformations to the data, since I need a DataFrame back, I have converted the result of RDD to DataFrame with new column names. Note that here I have used index to get the column values, alternatively, you can also refer to the DataFrame column names while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referring Column Names (another example)\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e197044",
   "metadata": {},
   "source": [
    "### Foreach examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9949c19",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#To get row by index. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mforeach(\u001b[38;5;28;01mlambda\u001b[39;00m x: \n\u001b[1;32m      3\u001b[0m               \u001b[38;5;28mprint\u001b[39m(x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m], x[\u001b[38;5;241m2\u001b[39m], x[\u001b[38;5;241m3\u001b[39m])) \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#To get columns.  \u001b[39;00m\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mforeach(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mprint\u001b[39m((x\u001b[38;5;241m.\u001b[39mcolumn1, x\u001b[38;5;241m.\u001b[39mcolumn2)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#To get row by index. \n",
    "df.rdd.foreach(lambda x: \n",
    "              print(x[0], x[1], x[2], x[3])) \n",
    "\n",
    "#To get columns.  \n",
    "df.rdd.foreach(lambda x: print((x.column1, x.column2)))\n",
    "\n",
    "#Functions:\n",
    "def f(row):\n",
    "    print(row.name)\n",
    "\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e82a7a",
   "metadata": {},
   "source": [
    "#  Create functions to iterate through rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a4b8f",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/how-to-loop-through-each-row-of-dataframe-in-pyspark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c782cb",
   "metadata": {},
   "source": [
    "PySpark provides map(), mapPartitions() to loop/iterate through rows in RDD/DataFrame to perform the complex transformations, and these two returns the same number of records as in the original DataFrame but the number of columns could be different (after add/update).\n",
    "\n",
    "PySpark also provides foreach() & foreachPartitions() actions to loop/iterate through each Row in a DataFrame but these two returns nothing, In this article, I will explain how to use these methods to get DataFrame column values and process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0933d38",
   "metadata": {},
   "source": [
    "### Method 1: Using collect()\n",
    "\n",
    "We can use collect() action operation for retrieving all the elements of the Dataset to the driver function then loop through it using for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675fea57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# retrieving all the elements\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# of the dataframe using collect()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Storing in the variable\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data_collect \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# looping thorough each row of the dataframe\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_collect:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# while looping through each\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# row printing the data of Id, Name and City\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# retrieving all the elements\n",
    "# of the dataframe using collect()\n",
    "# Storing in the variable\n",
    "data_collect = df.collect()\n",
    " \n",
    "# looping thorough each row of the dataframe\n",
    "for row in data_collect:\n",
    "    # while looping through each\n",
    "    # row printing the data of Id, Name and City\n",
    "    print(row[\"Id\"],row[\"Name\"],\"  \",row[\"City\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3d848",
   "metadata": {},
   "source": [
    "### Method 2: Using toLocalIterator()\n",
    "\n",
    "We can use toLocalIterator(). This returns an iterator that contains all the rows in the DataFrame. It is similar to collect(). The only difference is that collect() returns the list whereas toLocalIterator() returns an iterator.\n",
    "\n",
    "Note: This function is similar to collect() function as used in the above example the only difference is that this function returns the iterator whereas the collect() function returns the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ff141",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_itr = df.rdd.toLocalIterator()\n",
    " \n",
    "# looping thorough each row of the dataframe\n",
    "for row in data_itr:\n",
    "   \n",
    "    # while looping through each row printing\n",
    "    # the data of Id, Job Profile and City\n",
    "    print(row[\"Id\"],\" \",row[\"Job Profile\"],\"  \",row[\"City\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb660b",
   "metadata": {},
   "source": [
    "### Method 3: Using iterrows() (ONLY PANDAS)\n",
    "\n",
    "The iterrows() function for iterating through each row of the Dataframe, is the function of pandas library, so first, we have to convert the PySpark Dataframe into Pandas Dataframe using toPandas() function. Then loop through it using for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = df.toPandas()\n",
    " \n",
    "# looping through each row using iterrows()\n",
    "# used to iterate over dataframe rows as index,\n",
    "# series pair\n",
    "for index, row in pd_df.iterrows():\n",
    "   \n",
    "    # while looping through each row\n",
    "    # printing the Id, Name and Salary\n",
    "    # by passing index instead of Name\n",
    "    # of the column\n",
    "    print(row[0],row[1],\" \",row[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e9669",
   "metadata": {},
   "source": [
    "### Method 4: Using map()\n",
    "\n",
    "map() function with lambda function for iterating through each row of Dataframe. For looping through each row using map() first we have to convert the PySpark dataframe into RDD because map() is performed on RDD’s only, so first convert into RDD it then use map() in which, lambda function for iterating through each row and stores the new RDD in some variable then convert back that new RDD into Dataframe using toDF() by passing schema into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68139bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# function to create new SparkSession\n",
    "def create_session():\n",
    "  spk = SparkSession.builder \\\n",
    "      .master(\"local\") \\\n",
    "      .appName(\"employee_profile.com\") \\\n",
    "      .getOrCreate()\n",
    "  return spk\n",
    " \n",
    "def create_df(spark,data,schema):\n",
    "  df1 = spark.createDataFrame(data,schema)\n",
    "  return df1\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "  # calling function to create SparkSession\n",
    "  spark = create_session()\n",
    "     \n",
    "  input_data = [(1,\"Shivansh\",\"Data Scientist\",2000000,\"Noida\"),\n",
    "          (2,\"Rishabh\",\"Software Developer\",1500000,\"Banglore\"),\n",
    "          (3,\"Swati\",\"Data Analyst\",1000000,\"Hyderabad\"),\n",
    "          (4,\"Amar\",\"Data Analyst\",950000,\"Noida\"),\n",
    "          (5,\"Arpit\",\"Android Developer\",1600000,\"Pune\"),\n",
    "          (6,\"Ranjeet\",\"Python Developer\",1800000,\"Gurugram\"),\n",
    "          (7,\"Priyanka\",\"Full Stack Developer\",2200000,\"Banglore\")]\n",
    " \n",
    "  schema = [\"Id\",\"Name\",\"Job Profile\",\"Salary\",\"City\"]\n",
    " \n",
    "  # calling function to create dataframe\n",
    "  df = create_df(spark,input_data,schema)\n",
    " \n",
    "  # map() is only be performed on rdd\n",
    "  # so converting the dataframe into rdd using df.rdd\n",
    "  rdd = df.rdd.map(lambda loop: (\n",
    "      loop[\"Id\"],loop[\"Name\"],loop[\"Salary\"],loop[\"City\"])\n",
    "  )\n",
    " \n",
    "  # after looping the getting the data from each row\n",
    "  # converting back from RDD to Dataframe\n",
    "  df2 = rdd.toDF([\"Id\",\"Name\",\"Salary\",\"City\"])\n",
    " \n",
    "  # showing the new Dataframe\n",
    "  df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebda18",
   "metadata": {},
   "source": [
    "### Method 6: Using select()\n",
    "\n",
    "The select() function is used to select the number of columns. After selecting the columns, we are using the collect() function that returns the list of rows that contains only the data of selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69728e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# function to create new SparkSession\n",
    "def create_session():\n",
    "    spk = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"employee_profile.com\") \\\n",
    "        .getOrCreate()\n",
    "    return spk\n",
    " \n",
    " \n",
    "def create_df(spark, data, schema):\n",
    "    df1 = spark.createDataFrame(data, schema)\n",
    "    return df1\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    # calling function to create SparkSession\n",
    "    spark = create_session()\n",
    " \n",
    "    input_data = [(1, \"Shivansh\", \"Data Scientist\", 2000000, \"Noida\"),\n",
    "                  (2, \"Rishabh\", \"Software Developer\", 1500000, \"Banglore\"),\n",
    "                  (3, \"Swati\", \"Data Analyst\", 1000000, \"Hyderabad\"),\n",
    "                  (4, \"Amar\", \"Data Analyst\", 950000, \"Noida\"),\n",
    "                  (5, \"Arpit\", \"Android Developer\", 1600000, \"Pune\"),\n",
    "                  (6, \"Ranjeet\", \"Python Developer\", 1800000, \"Gurugram\"),\n",
    "                  (7, \"Priyanka\", \"Full Stack Developer\", 2200000, \"Banglore\")]\n",
    " \n",
    "    schema = [\"Id\", \"Name\", \"Job Profile\", \"Salary\", \"City\"]\n",
    " \n",
    "    # calling function to create dataframe\n",
    "    df = create_df(spark, input_data, schema)\n",
    " \n",
    "    # getting each row of dataframe containing\n",
    "    # only selected columns Selected columns are\n",
    "    # 'Name' and 'Salary' getting the list of rows\n",
    "    # with selected column data using collect()\n",
    "    rows_looped = df.select(\"Name\", \"Salary\").collect()\n",
    " \n",
    "    # printing the data of each row\n",
    "    for rows in rows_looped:\n",
    "       \n",
    "        # here index 0 and 1 refers to the data\n",
    "        # of 'Name' column and 'Salary' column\n",
    "        print(rows[0], rows[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efe957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
