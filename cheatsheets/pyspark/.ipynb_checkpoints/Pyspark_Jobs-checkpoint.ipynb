{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e833c774",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6827d7b",
   "metadata": {},
   "source": [
    "For starters, to intialize spark sessions:\n",
    "https://www.elenacuoco.com/2016/08/28/pyspark-first-approaches-ml-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/vieroh/apps/spark-3.3.0-bin-hadoop3-scala2.13') #Here your spark rute.\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option('header', 'true')\n",
    "          .load(\"YOUR.csv\"))\n",
    "          \n",
    "print((df.count(), len(df.columns))) #Shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60363e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession sparkSession =SparkSession.builder() \n",
    ".master(\"local\") \n",
    ".appName(\"Spark Session Example\") \n",
    ".getOrCreate(); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011dbe7b",
   "metadata": {},
   "source": [
    "### RDD to DF & DF to RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d802272",
   "metadata": {},
   "source": [
    "If you want to convert between RDD / Dataframe (Dataset) use methods which are designed to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afc5e77",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3998269930.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    import org.apache.spark.sql.DataFrame\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# from DataFrame to RDD:\n",
    "\n",
    "    import org.apache.spark.sql.DataFrame\n",
    "    import org.apache.spark.sql.Row\n",
    "    import org.apache.spark.rdd.RDD\n",
    "\n",
    "    DataFrame  = Seq((\"foo\", 1), (\"bar\", 2)).toDF(\"k\", \"v\")\n",
    "    RDD[Row] = df.rdd\n",
    "\n",
    "# form RDD to DataFrame:\n",
    "\n",
    "    val rdd: RDD[(String, Int)] = sc.parallelize(Seq((\"foo\", 1), (\"bar\", 2)))\n",
    "    val df1: DataFrame = rdd.toDF\n",
    "    # or\n",
    "    val df2: DataFrame = spark.createDataFrame(rdd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974aed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264e845b",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9110bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "#Count and show how many \"isnull\" are per column\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()\n",
    "\n",
    "# Fill missing values with another:\n",
    "dataset = dataset.select(\"Age\").na.fill(\"28\")   \n",
    "\n",
    "#Drop every row with missing values (careful, you could end with no rows at all).\n",
    "dataset = dataset.replace('null', None).dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f44bb4",
   "metadata": {},
   "source": [
    "## Batch value changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#Nulls and replacing values\n",
    ">>> df.na.fill(50).show() #Replace ALL null values in all columns\n",
    ">>> df.na.drop().show() #Return new df omitting rows with null values\n",
    ">>> df.na.replace(10, 20).show()   #Return new df replacing one value with another\n",
    "\n",
    "# Batch change of type of Data per column:\n",
    "df_good_types = df.select(col('Column_1').cast('float'),\n",
    "                         col('Column_2').cast('float'), ...)\n",
    "                         \n",
    "# Batch change of categories (categorical columns), from categories to integers:\n",
    "dataset = StringIndexer(\n",
    "    inputCol='Gender', \n",
    "    outputCol='GenderIndex', \n",
    "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "\n",
    "# Batch Category-to-Number transformation with One Hot Encoding, for not-quantitative variables:\n",
    "dataset = StringIndexer(\n",
    "    inputCol='Geography', \n",
    "    outputCol='GeographyNum', \n",
    "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "\n",
    "dataset = OneHotEncoder(\n",
    "    inputCol='GeographyNum', \n",
    "    outputCol='GeographyIndex', \n",
    "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
    "       \n",
    "       \n",
    "# Specific value changing by column and conditions (with Lists and \"when\")\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "x = ['ValA','ValB','ValF']\n",
    "y = ['ValM','ValR']\n",
    "z = ['ValP','ValS']\n",
    "\n",
    "df = df.withColumn('targetCol', when(col('targetCol').isin(x), \"x\")\\\n",
    "                  .otherwise(when(col('targetCol').isin(y), \"y\")\\\n",
    "                  .otherwise(when(col('targetCol').isin(z), \"z\")\\\n",
    "                  .otherwise(df.newColumn))))\n",
    "\n",
    "# Specific string value changing by column and conditions (with Dicts and Replace)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "replacement = {\n",
    "    \"LB\": \"x\", \"LWB\": \"x\", \"LF\": \"x\",\n",
    "    \"RF\": \"y\", \"LCM\": \"y\",\n",
    "    \"LM\": \"z\", \"RS\": \"z\"\n",
    "}\n",
    "\n",
    "df1 = df.replace(replacement, [\"targetCol\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abac719",
   "metadata": {},
   "source": [
    "##  MachineLearning_Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble all the features with VectorAssembler. Take all input features, all the columns but the one you are trying to gess:\n",
    "\n",
    "required_features = ['Column_1','Column_2',...]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "transformed_data = assembler.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2fabc6",
   "metadata": {},
   "source": [
    "# SQL Perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0b95f",
   "metadata": {},
   "source": [
    "- > https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/\n",
    "\n",
    "- > https://www.projectpro.io/recipes/explain-spark-sql-withcolumn-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16edd229",
   "metadata": {},
   "source": [
    "# Fill and replace Null, NaN, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f2e2b",
   "metadata": {},
   "source": [
    "- > https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca1db0",
   "metadata": {},
   "source": [
    "# Replace Column Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b63d81",
   "metadata": {},
   "source": [
    "- > https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcadfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46796d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
